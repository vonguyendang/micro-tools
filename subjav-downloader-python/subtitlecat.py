import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse
import time
import re
from datetime import datetime, timedelta

def find_subtitle_url(url):
    """
    T√¨m URL t·∫£i xu·ªëng ph·ª• ƒë·ªÅ ti·∫øng Vi·ªát t·ª´ trang web.

    Args:
        url (str): URL c·ªßa trang t√¨m ki·∫øm.

    Returns:
        str: URL t·∫£i xu·ªëng ph·ª• ƒë·ªÅ ti·∫øng Vi·ªát, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y.
    """
    try:
        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")
        td_tags = soup.find_all("td")
        td_tags_with_href = [td for td in td_tags if td.find("a") and td.find("a").has_attr("href")]

        hrefs = [td.find("a")["href"] for td in td_tags_with_href[:3] if td.find("a")]  # L·∫•y tr·ª±c ti·∫øp href

        subtitle_code = url.split("=")[-1]
        subtitle_url = None

        for href in hrefs:
            if href and re.search(re.escape(subtitle_code), href, re.IGNORECASE):
                subtitle_url = "https://subtitlecat.com/" + href
                print(f"‚úîÔ∏è T√¨m th·∫•y: subtitle_code={subtitle_code}, href={href}, subtitle_url={subtitle_url}")
                return subtitle_url

        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c·∫£ hai chu·ªói
        code_found = any(re.search(re.escape(subtitle_code), h, re.IGNORECASE) for h in hrefs if h)
        aldn_found = any(re.search(re.escape("ALDN-332"), h, re.IGNORECASE) for h in hrefs if h)

        if not code_found and not aldn_found:
            print(f"‚ùå C·∫£ {subtitle_code} v√† ALDN-332 ƒë·ªÅu kh√¥ng c√≥ trong URL.")
            return "NOT_FOUND"  # S·ª≠ d·ª•ng m·ªôt gi√° tr·ªã ƒë·∫∑c bi·ªát

        return None  # Tr·∫£ v·ªÅ None n·∫øu ch·ªâ c√≥ ALDN-332 ho·∫∑c kh√¥ng t√¨m th·∫•y g√¨ kh√°c

    except requests.exceptions.RequestException as e:
        print(f"‚ùå L·ªói request khi x·ª≠ l√Ω {url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω {url}: {e}")
        return None


def get_download_link(url):
    """
    L·∫•y URL t·∫£i xu·ªëng th·ª±c t·∫ø t·ª´ trang trung gian.

    Args:
        url (str): URL trang ch·ª©a li√™n k·∫øt t·∫£i xu·ªëng.

    Returns:
        str: URL t·∫£i xu·ªëng tr·ª±c ti·∫øp, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y.
    """
    try:
        subtitle_response = requests.get(url)
        subtitle_response.raise_for_status()

        subtitle_soup = BeautifulSoup(subtitle_response.content, "html.parser")
        download_link = subtitle_soup.find("a", id="download_vi")
        if download_link and download_link.has_attr("href"):
            download_url = "https://subtitlecat.com" + download_link["href"]
            print(f"‚ûï ƒê√£ t·∫°o URL t·∫£i xu·ªëng ph·ª• ƒë·ªÅ: {download_url}")
            return download_url
        else:
            print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y URL t·∫£i xu·ªëng ph·ª• ƒë·ªÅ.")
            return None

    except requests.exceptions.RequestException as e:
        print(f"‚ùå L·ªói request khi l·∫•y link t·∫£i xu·ªëng t·ª´ {url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi l·∫•y link t·∫£i xu·ªëng t·ª´ {url}: {e}")
        return None

def format_timedelta(td):
    """
    ƒê·ªãnh d·∫°ng ƒë·ªëi t∆∞·ª£ng timedelta th√†nh chu·ªói c√≥ d·∫°ng "HH:MM:SS".

    Args:
        td (timedelta): ƒê·ªëi t∆∞·ª£ng timedelta c·∫ßn ƒë·ªãnh d·∫°ng.

    Returns:
        str: Chu·ªói ƒë·ªãnh d·∫°ng th·ªùi gian.
    """
    hours, remainder = divmod(td.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    return "{:02}:{:02}:{:02}".format(int(hours), int(minutes), int(seconds))

def download_subtitle(url, folder=".", filename="subtitle.srt"):
    """
    T·∫£i xu·ªëng ph·ª• ƒë·ªÅ t·ª´ URL ƒë√£ cho v√† l∆∞u v√†o m·ªôt t·ªáp trong th∆∞ m·ª•c ƒë√£ ch·ªâ ƒë·ªãnh.

    Args:
        url (str): URL t·∫£i ph·ª• ƒë·ªÅ.
        folder (str): Th∆∞ m·ª•c ƒë·ªÉ t·∫£i file, n·∫øu th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i s·∫Ω t·∫°o m·ªõi.
        filename (str): T√™n file ƒë·ªÉ l∆∞u ph·ª• ƒë·ªÅ.

    Returns:
        bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i.
    """
    try:
        srt_url = get_download_link(url)
        if not srt_url:
            return False  # Kh√¥ng t·∫£i n·∫øu kh√¥ng c√≥ URL

        if not os.path.exists(folder):
            os.makedirs(folder)
            print(f"‚ûï ƒê√£ t·∫°o th∆∞ m·ª•c: {folder}")

        response = requests.get(srt_url)
        response.raise_for_status()

        filepath = os.path.join(folder, filename)

        with open(filepath, "wb") as f:
            f.write(response.content)
        print(f"‚¨áÔ∏è üíæ Ph·ª• ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c t·∫£i xu·ªëng v√† l∆∞u v√†o {filepath}")
        return True

    except requests.exceptions.RequestException as e:
        print(f"‚ùå L·ªói request khi t·∫£i xu·ªëng t·ª´ {srt_url}: {e}")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i xu·ªëng t·ª´ {srt_url}: {e}")
        return False


def main():
    """
    ƒê·ªçc URL t·ª´ sublink.txt, t·∫£i xu·ªëng ph·ª• ƒë·ªÅ v√† l∆∞u v·ªõi t√™n l·∫•y t·ª´ URL.
    """

    try:
        with open("sublink.txt", "r") as url_file:
            url_lines = [url.strip() for url in url_file.readlines()]
        print("‚úîÔ∏è ƒê√£ ƒë·ªçc URL t·ª´ file sublink.txt.")
    except FileNotFoundError:
        print("‚ùå L·ªói : Kh√¥ng t√¨m th·∫•y file sublink.txt")
        return

    try:
        with open("name.txt", "r") as name_file:
            name_lines = [name.strip() for name in name_file.readlines()]
        print("‚úîÔ∏è ƒê√£ ƒë·ªçc t√™n t·ª´ file name.txt.")
    except FileNotFoundError:
        print("‚ùå L·ªói : Kh√¥ng t√¨m th·∫•y file name.txt")
        return

    if len(url_lines) != len(name_lines):
        print("‚ö†Ô∏è L·ªói: S·ªë l∆∞·ª£ng URL v√† t√™n kh√¥ng kh·ªõp.")
        return

    folder = "subtitles"
    total_urls = len(url_lines)
    max_download_retries = 20
    url_retry_counts = {}
    not_found_urls = []
    cannot_download_urls = []
    successful_downloads = 0


    start_time = datetime.now()  # Ghi l·∫°i th·ªùi gian b·∫Øt ƒë·∫ßu
    
   
    print()
    print(f"üí• B·∫Øt ƒë·∫ßu l√∫c: {start_time.strftime('%H:%M:%S %d/%m/%Y')}")

    for i, url in enumerate(url_lines):


        processed_count = i + 1

        elapsed_time = datetime.now() - start_time  # T√≠nh th·ªùi gian ƒë√£ tr√¥i qua
        avg_time_per_url = elapsed_time / processed_count if processed_count > 0 else timedelta(0)  # T√≠nh th·ªùi gian trung b√¨nh
        remaining_time = avg_time_per_url * (total_urls - processed_count)  # T√≠nh th·ªùi gian c√≤n l·∫°i
        end_time_est = datetime.now() + remaining_time  # ∆Ø·ªõc t√≠nh th·ªùi gian k·∫øt th√∫c
        # T√≠nh to√°n t·ª∑ l·ªá t·∫°m th·ªùi v√† in ra
        temp_success_rate = (successful_downloads / (processed_count-1)) * 100 if processed_count-1 else 0
        temp_not_found_rate = (len(not_found_urls) / (processed_count-1)) * 100 if processed_count-1 else 0
        temp_failed_download_rate = (len(cannot_download_urls) / (processed_count-1)) * 100 if processed_count-1 else 0

        print()
        print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
        print()
        print(f"‚è±Ô∏è ƒê√£ th·ª±c hi·ªán ƒë∆∞·ª£c: {format_timedelta(elapsed_time)}, D·ª± ki·∫øn ho√†n th√†nh sau {format_timedelta(remaining_time)} , l√∫c: {end_time_est.strftime('%H:%M:%S')}")
        print(f"üí° Th√†nh c√¥ng: {temp_success_rate:.2f}% ({successful_downloads} URLS) | Kh√¥ng t√¨m th·∫•y: {temp_not_found_rate:.2f}% ({len(not_found_urls)} URLS) | Kh√¥ng th·ªÉ t·∫£i: {temp_failed_download_rate:.2f}% ({len(cannot_download_urls)} URLS)")
        print(f"üöÄ ƒêang x·ª≠ l√Ω URL th·ª© {processed_count}/{total_urls}: {url}")

        download_url = None
        for retry in range(max_download_retries):
            print(f"üîÑ L·∫ßn th·ª≠: {retry + 1} - T√¨m URL t·ª´: {url}")
            code = url.split("=")[-1]
            download_url = find_subtitle_url(url)
            if download_url == "NOT_FOUND":
                break  # D·ª´ng th·ª≠ l·∫°i n·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng t√¨m th·∫•y
            elif download_url:
                break  # D·ª´ng th·ª≠ l·∫°i n·∫øu t√¨m th·∫•y
            time.sleep(3)

        if download_url and download_url != "NOT_FOUND":
            filename = name_lines[i]  # T·∫°o t√™n file t·ª´ name.txt
            if download_subtitle(download_url, folder, filename):
                successful_downloads += 1
                pass
            else:
                print(f"‚ö†Ô∏è L·ªói: Kh√¥ng th·ªÉ t·∫£i xu·ªëng ph·ª• ƒë·ªÅ t·ª´ {download_url}, ghi v√†o cannotdownload.txt")
                cannot_download_urls.append((download_url, url, name_lines[i]))
        elif download_url == "NOT_FOUND":
            print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y ALDN-332 v√† {code} trong URL: {url}, ghi v√†o notfound.txt")
            not_found_urls.append((code, url, name_lines[i]))
        else:
            print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y URL t·∫£i xu·ªëng cho {url}, ghi v√†o notfound.txt")
            not_found_urls.append((code, url, name_lines[i]))

        time.sleep(3)

    if not_found_urls:
        with open("notfound.txt", "w") as f:
            for code, url, name in not_found_urls:
                f.write(f"‚ùå Kh√¥ng t√¨m th·∫•y srt cho code: {code} ; url: {url} ; name: {name}\n")
        print()
        print("‚úÖ ƒê√£ ghi c√°c URL kh√¥ng t√¨m th·∫•y v√†o notfound.txt")

    if cannot_download_urls:
        with open("cannotdownload.txt", "w") as f:
            for download_url, url, name in cannot_download_urls:
                f.write(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i srt t·ª´: {download_url} ; url: {url} ; name: {name}\n")
        print("‚úÖ ƒê√£ ghi c√°c URL kh√¥ng th·ªÉ t·∫£i v√†o cannotdownload.txt")

    print("‚úÖ ƒê√£ x·ª≠ l√Ω xong t·∫•t c·∫£ c√°c URL.")
        # Th√™m th√¥ng tin th·ªëng k√™
    success_rate = (successful_downloads / total_urls) * 100 if total_urls else 0
    not_found_rate = (len(not_found_urls) / total_urls) * 100 if total_urls else 0
    failed_download_rate = (len(cannot_download_urls) / total_urls) * 100 if total_urls else 0
 
    print()
    print("--- Th·ªëng k√™ ---")
    print(f"T·ªïng URL x·ª≠ l√Ω: {total_urls} URLS")
    print(f"T·ª∑ l·ªá th√†nh c√¥ng: {success_rate:.2f}% ({successful_downloads} URLS)")
    print(f"T·ª∑ l·ªá kh√¥ng t√¨m th·∫•y: {not_found_rate:.2f}% ({len(not_found_urls)} URLS)")
    print(f"T·ª∑ l·ªá kh√¥ng th·ªÉ t·∫£i xu·ªëng: {failed_download_rate:.2f}% ({len(cannot_download_urls)} URLS)")
    print()
    print(f"üí• K·∫øt th√∫c: {datetime.now().strftime('%H:%M:%S %d/%m/%Y')}")


if __name__ == "__main__":
    main()